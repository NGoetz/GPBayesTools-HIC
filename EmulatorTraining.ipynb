{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: dill in /home/goetz/.local/lib/python3.10/site-packages (0.3.8)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: surmise in /home/goetz/.local/lib/python3.10/site-packages (0.2.1)\n",
      "Requirement already satisfied: scikit-learn>=1.2.0 in /home/goetz/.local/lib/python3.10/site-packages (from surmise) (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.18.3 in /usr/lib/python3/dist-packages (from surmise) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.7 in /usr/lib/python3/dist-packages (from surmise) (1.8.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/goetz/.local/lib/python3.10/site-packages (from scikit-learn>=1.2.0->surmise) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/lib/python3/dist-packages (from scikit-learn>=1.2.0->surmise) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "!pip install dill\n",
    "!pip install surmise\n",
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "import dill\n",
    "import pickle\n",
    "sys.path.insert(0, path.abspath('./'))\n",
    "\n",
    "from src.emulator_BAND import EmulatorBAND\n",
    "#from src.emulator import Emulator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to create pickle file with training dataset and validation test points\n",
    "\n",
    "Use the last 5 parameters as validation. Later we might add posterior points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(file):\n",
    "    with open(file, 'r') as f:\n",
    "        # Initialize an empty dictionary to store the values\n",
    "        values = []\n",
    "        # Read the file line by line\n",
    "        for line in f:\n",
    "            # If the line starts with a '#', it's a comment\n",
    "            if line.startswith('#'):\n",
    "                # Split the line at the colon\n",
    "                parts = line.split(':')\n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "                # The key is the part before the colon, stripped of leading/trailing whitespace and the '#'\n",
    "                key = parts[0].strip().lstrip('#')\n",
    "                # The value is the part after the colon, stripped of leading/trailing whitespace\n",
    "                value = float(parts[1].strip())\n",
    "                # Add the key-value pair to the dictionary\n",
    "                values.append(value)\n",
    "    return values\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "####################\n",
    "#GLOBAL VARIABLES\n",
    "num_points=500\n",
    "#num_points=4\n",
    "####################\n",
    "parent=\"../actual\"\n",
    "\n",
    "\n",
    "#generate the data pickles\n",
    "path_data = parent+'/merged_directory'\n",
    "path_configs = parent+'/configs'\n",
    "path_output = parent+'/latent_pickled/'\n",
    "path_output_exp = parent+'/exp/'\n",
    "path_input_exp=parent+'/exp_input/0'\n",
    "datasets = [\n",
    "    'exp_19.6_05_eta_spectra', 'exp_200_05_y_spectra_piminus',\n",
    "    'exp_19.6_05_integrated', 'exp_200_05_y_spectra_piplus',\n",
    "    'exp_19.6_05_pT_spectra_kminus', 'exp_200_1525_eta_spectra',\n",
    "    'exp_19.6_05_pT_spectra_kplus', 'exp_200_1525_phobos_v2_spectra',\n",
    "    'exp_19.6_05_pT_spectra_p', 'exp_200_2030_integrated',\n",
    "    'exp_19.6_05_pT_spectra_pbar', 'exp_200_2030_phenix_pT_v2_spectra',\n",
    "    'exp_19.6_05_pT_spectra_piminus', 'exp_200_2030_phenix_pT_v3_spectra',\n",
    "    'exp_19.6_05_pT_spectra_piplus', 'exp_200_2030_pT_spectra_kminus',\n",
    "    'exp_19.6_1525_eta_spectra', 'exp_200_2030_pT_spectra_kplus','exp_200_2030_pT_spectra_p',\n",
    "    'exp_19.6_2030_integrated', 'exp_200_2030_pT_spectra_pbar',\n",
    "    'exp_19.6_2030_pT_spectra_kminus', 'exp_200_2030_pT_spectra_piminus',\n",
    "    'exp_19.6_2030_pT_spectra_kplus', 'exp_200_2030_pT_spectra_piplus',\n",
    "    'exp_19.6_2030_pT_spectra_p', 'exp_7.7_05_integrated',\n",
    "    'exp_19.6_2030_pT_spectra_pbar', 'exp_7.7_05_pT_spectra_kminus',\n",
    "    'exp_19.6_2030_pT_spectra_piminus', 'exp_7.7_05_pT_spectra_kplus',\n",
    "    'exp_19.6_2030_pT_spectra_piplus', 'exp_7.7_05_pT_spectra_p',\n",
    "    'exp_19.6_2030_star_v2_pT_spectra', 'exp_7.7_05_pT_spectra_pbar',\n",
    "    'exp_200_05_eta_spectra', 'exp_7.7_05_pT_spectra_piminus',\n",
    "    'exp_200_05_integrated', 'exp_7.7_05_pT_spectra_piplus',\n",
    "    'exp_200_05_pT_spectra_kminus', 'exp_7.7_2030_integrated',\n",
    "    'exp_200_05_pT_spectra_kplus', 'exp_7.7_2030_pT_spectra_kminus',\n",
    "    'exp_200_05_pT_spectra_p', 'exp_7.7_2030_pT_spectra_kplus',\n",
    "    'exp_200_05_pT_spectra_pbar', 'exp_7.7_2030_pT_spectra_p',\n",
    "    'exp_200_05_pT_spectra_piminus', 'exp_7.7_2030_pT_spectra_pbar',\n",
    "    'exp_200_05_pT_spectra_piplus', 'exp_7.7_2030_pT_spectra_piminus',\n",
    "    'exp_200_05_y_spectra_kminus', 'exp_7.7_2030_pT_spectra_piplus',\n",
    "    'exp_200_05_y_spectra_kplus', 'exp_7.7_2030_star_v2_pT_spectra'\n",
    "]\n",
    "integrated_values=[\n",
    "    \"dNdy_kminus\",\n",
    "    \"dNdy_kplus\",\n",
    "    \"dNdy_p\",\n",
    "    \"dNdy_pbar\",\n",
    "    \"dNdy_piminus\",\n",
    "    \"dNdy_piplus\",\n",
    "    \"meanpT_kminus\",\n",
    "    \"meanpT_kplus\",\n",
    "    \"meanpT_p\",\n",
    "    \"meanpT_pbar\",\n",
    "    \"meanpT_piminus\",\n",
    "    \"meanpT_piplus\",\n",
    "    \"star_v2\",\n",
    "    \"star_v3\"\n",
    "]\n",
    " \n",
    "\n",
    "import random \n",
    "\n",
    "# flags if certain sets should be used or not\n",
    "def pickle_data_in_from_to(path_output, start, end,  outname,eta_cut=None, data=\"all\"):\n",
    "    #print(\"RAndom variation added!\")\n",
    "    dict_df_full={}\n",
    "    match data:\n",
    "        case \"base_wo_phobos\":\n",
    "            datasets_local = [d for d in datasets if (\"phobos\" not in d and \"eta\" not in d and \"y_spectra\" not in d and \"pT_spectra\" not in d and \"phenix\" not in d and \"star_v2_pT_spectra\" not in d)]\n",
    "        case \"base_200_only\":\n",
    "            datasets_local = [d for d in datasets if (\"200\" in d and \"phobos\" not in d and \"eta\" not in d and \"y_spectra\" not in d and \"pT_spectra\" not in d and \"phenix\" not in d and \"star_v2_pT_spectra\" not in d)]\n",
    "        case \"base_200_and_phobos\":\n",
    "            datasets_local = [d for d in datasets if (\"200\" in d and \"y_spectra\" not in d and \"pT_spectra\" not in d and \"phenix\" not in d and \"star_v2_pT_spectra\" not in d)]\n",
    "        case \"base\":\n",
    "           datasets_local = [d for d in datasets if (\"y_spectra\" not in d and \"pT_spectra\" not in d and \"phenix\" not in d and \"star_v2_pT_spectra\" not in d)]\n",
    "        case \"base_and_pT_spectra\":\n",
    "           datasets_local = [d for d in datasets if (\"y_spectra\" not in d and \"phenix\" not in d and \"star_v2_pT_spectra\" not in d)] \n",
    "        case \"base_and_y_spectra\":\n",
    "            datasets_local = [d for d in datasets if ( \"pT_spectra\" not in d and \"phenix\" not in d and \"star_v2_pT_spectra\" not in d)]\n",
    "        case \"base_and_phenix\":\n",
    "            datasets_local = [d for d in datasets if (\"y_spectra\" not in d and \"pT_spectra\" not in d  and \"star_v2_pT_spectra\" not in d)]\n",
    "        case \"base_and_v2pT\":\n",
    "            datasets_local = [d for d in datasets if (\"y_spectra\" not in d and \"pT_spectra\" not in d and \"phenix\" not in d)]\n",
    "        case _:\n",
    "            datasets_local = datasets.copy()\n",
    "    for i in range(start, end):\n",
    "        i_str = str(i+1).zfill(3)\n",
    "        dict_df={\"obs\": [], \"parameter\": [], \"name\":[], \"lim\":[0]}\n",
    "        config_path = path_configs +\"/config_sets_run_\"+str(i+1).zfill(3)+\".yaml\"\n",
    "        dict_df[\"parameter\"]=read_config(config_path)\n",
    "        obs_1 = []  # List for first entries of tuples\n",
    "        obs_2 = []  # List for second entries of tuples\n",
    "        for dataset in datasets_local:\n",
    "            if \"integrated\" not in dataset:\n",
    "                current_path = path_data +\"/\"+i_str+\"/\"+dataset.replace(\"exp\", i_str)\n",
    "                \n",
    "                df = pd.read_csv(current_path)\n",
    "                if eta_cut is not None and (\"eta\" in dataset or \"y_spectra\" in dataset or \"phobos\" in dataset):\n",
    "                    df_filtered = df[abs(df.iloc[:, -3]) < eta_cut]\n",
    "                    obs_1.extend(df_filtered.iloc[:, -2])\n",
    "                    obs_2.extend(df_filtered.iloc[:, -1])\n",
    "                    dict_df[\"lim\"].append(df_filtered.shape[0]+dict_df[\"lim\"][-1])\n",
    "                else:   \n",
    "                    obs_1.extend(df.iloc[:, -2])\n",
    "                    obs_2.extend(df.iloc[:, -1])\n",
    "                    dict_df[\"lim\"].append(df.shape[0]+dict_df[\"lim\"][-1])\n",
    "                \n",
    "                dict_df[\"name\"].append(dataset)\n",
    "                \n",
    "            elif \"integrated\" in dataset:\n",
    "                for key in integrated_values:\n",
    "                    current_path = path_data +\"/\"+i_str+\"/\"+dataset.replace(\"exp\", i_str)   \n",
    "                    df = pd.read_csv(current_path)\n",
    "                    obs_1.extend(df[key].values*random.uniform(0.8, 2))\n",
    "                    obs_2.extend(df[key+\"_error\"].values)\n",
    "                    dict_df[\"name\"].append(dataset+\"_\"+key)\n",
    "                    dict_df[\"lim\"].append(df.shape[0]+dict_df[\"lim\"][-1])\n",
    "        dict_df[\"obs\"] = np.vstack((obs_1, obs_2))\n",
    "        dict_df_full[i]=dict_df\n",
    "    with open( path_output+outname+\".pkl\", 'wb') as f:\n",
    "        pickle.dump(dict_df_full, f)\n",
    "        \n",
    "def pickle_data_exp(path_output, outname,eta_cut=None,data=\"all\"):\n",
    "    dict_df_full={}\n",
    "    dict_df={\"obs\": [],  \"name\":[], \"lim\":[0]}\n",
    "    obs_1 = []  # List for first entries of tuples\n",
    "    obs_2 = []  # List for second entries of tuples\n",
    "    match data:\n",
    "        case \"base_wo_phobos\":\n",
    "            datasets_local = [d for d in datasets if (\"phobos\" not in d and \"eta\" not in d and \"y_spectra\" not in d and \"pT_spectra\" not in d and \"phenix\" not in d and \"star_v2_pT_spectra\" not in d)]\n",
    "        case \"base_200_only\":\n",
    "            datasets_local = [d for d in datasets if (\"200\" in d and \"phobos\" not in d and \"eta\" not in d and \"y_spectra\" not in d and \"pT_spectra\" not in d and \"phenix\" not in d and \"star_v2_pT_spectra\" not in d)]\n",
    "        case \"base_200_and_phobos\":\n",
    "            datasets_local = [d for d in datasets if (\"200\" in d and \"y_spectra\" not in d and \"pT_spectra\" not in d and \"phenix\" not in d and \"star_v2_pT_spectra\" not in d)]\n",
    "        case \"base\":\n",
    "            datasets_local = [d for d in datasets if (\"y_spectra\" not in d and \"pT_spectra\" not in d and \"phenix\" not in d and \"star_v2_pT_spectra\" not in d)]\n",
    "        case \"base_and_pT_spectra\":\n",
    "            datasets_local = [d for d in datasets if (\"y_spectra\" not in d and \"phenix\" not in d and \"star_v2_pT_spectra\" not in d)] \n",
    "        case \"base_and_y_spectra\":\n",
    "            datasets_local = [d for d in datasets if ( \"pT_spectra\" not in d and \"phenix\" not in d and \"star_v2_pT_spectra\" not in d)]\n",
    "        case \"base_and_phenix\":\n",
    "            datasets_local = [d for d in datasets if (\"y_spectra\" not in d and \"pT_spectra\" not in d  and \"star_v2_pT_spectra\" not in d)]\n",
    "        case \"base_and_v2pT\":\n",
    "            datasets_local = [d for d in datasets if (\"y_spectra\" not in d and \"pT_spectra\" not in d and \"phenix\" not in d)]\n",
    "        case _:\n",
    "            datasets_local = datasets.copy()\n",
    "    for dataset in datasets_local:\n",
    "        if \"integrated\" not in dataset:\n",
    "            current_path = path_input_exp+\"/\"+dataset\n",
    "            df = pd.read_csv(current_path)\n",
    "            if eta_cut is not None and (\"eta\" in dataset or \"y_spectra\" in dataset or \"phobos\" in dataset):\n",
    "                df_filtered = df[abs(df.iloc[:, -3]) < eta_cut]\n",
    "                obs_1.extend(df_filtered.iloc[:, -2])\n",
    "                obs_2.extend(df_filtered.iloc[:, -1])\n",
    "                dict_df[\"lim\"].append(df_filtered.shape[0]+dict_df[\"lim\"][-1])\n",
    "            else:\n",
    "                obs_1.extend(df.iloc[:, -2])\n",
    "                obs_2.extend(df.iloc[:, -1])\n",
    "            \n",
    "            dict_df[\"name\"].append(dataset)\n",
    "            dict_df[\"lim\"].append(df.shape[0]+dict_df[\"lim\"][-1])\n",
    "        elif \"integrated\" in dataset:\n",
    "            for key in integrated_values:\n",
    "                current_path = path_input_exp+\"/\"+dataset   \n",
    "                df = pd.read_csv(current_path)\n",
    "                obs_1.extend(df[key].values)\n",
    "                obs_2.extend(df[key+\"_error\"].values)\n",
    "                dict_df[\"name\"].append(dataset+\"_\"+key)\n",
    "                dict_df[\"lim\"].append(df.shape[0]+dict_df[\"lim\"][-1])\n",
    "    dict_df[\"obs\"] = np.vstack((obs_1, obs_2))\n",
    "    dict_df_full[0]=dict_df\n",
    "    with open( path_output+outname+\".pkl\", 'wb') as f:\n",
    "        pickle.dump(dict_df_full, f)\n",
    "pickle_data_exp(path_output_exp,  \"exp\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do they have to be pickels?\n",
    "#the pickels contain dictonaries. the dictionary is [{obs: [[val1, val2...], [err1,err2...]], parameter: i}] with length design points\n",
    "\"\"\"\n",
    "dataDict = {\n",
    "    'event_id1': { #id of parameter points\n",
    "        'obs': [[...], [...]],  # 2D array-like structure, values and errors\n",
    "        'parameter': ...  # parameter list, just pure numerical values\n",
    "    },\n",
    "    'event_id2': {\n",
    "        'obs': [[...], [...]],  # 2D array-like structure\n",
    "        'parameter': ...  # Scalar or 1D array\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "\"\"\"\n",
    "#print(\"The following pickles have been created:\")\n",
    "#datasets = list_of_pickles\n",
    "path_output_train = parent+'/latent_train/'\n",
    "path_output_val = parent+'/latent_val/'\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output_train):\n",
    "    os.makedirs(path_output_train)\n",
    "\n",
    "if not os.path.exists(path_output_val ):\n",
    "    os.makedirs(path_output_val )\n",
    "\n",
    "def check_file_length(filename, expected_length):\n",
    "    with open(f\"{path_output}{filename}\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    if len(data) == expected_length:\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"{filename} does not have the correct length. Expected: {expected_length}, Actual: {len(data)}\")\n",
    "\n",
    "# datasets_train = []\n",
    "# for dataset in datasets:\n",
    "#     current_path = dataset\n",
    "#     #print(dataset)\n",
    "#     with open(current_path, \"rb\") as pf:\n",
    "#         data = pickle.load(pf)\n",
    "    \n",
    "#     #print(f\"The dataset {dataset} has length {len(data)}.\")\n",
    "#     # Separate data based on event ID\n",
    "#     sorted_event_ids = sorted(data.keys(), key=lambda x: int(x))\n",
    "#     first_data = {event_id: data[event_id] for event_id in sorted_event_ids[:num_points-1]}\n",
    "#     second_data = {event_id: data[event_id] for event_id in sorted_event_ids[num_points-1:]}\n",
    "#     #print(f'{dataset.split(\".p\")[0].split(\"/\")[-1]}_train.pkl')\n",
    "#     # Save separated data to pickle files\n",
    "#     with open(f'{path_output_train}{dataset.split(\".p\")[0].split(\"/\")[-1]}_train.pkl', 'wb') as pf1:\n",
    "#         pickle.dump(first_data, pf1)\n",
    "        \n",
    "#     with open(f'{path_output_val}{dataset.split(\".p\")[0].split(\"/\")[-1]}_val.pkl', 'wb') as pf2:\n",
    "#         pickle.dump(second_data, pf2)\n",
    "\n",
    "#     check_file_length(f'{path_output_train}{dataset.split(\".p\")[0].split(\"/\")[-1]}_train.pkl', num_points-1)\n",
    "#     datasets_train.append(f'{path_output_train}{dataset.split(\".p\")[0].split(\"/\")[-1]}_train.pkl')\n",
    "#     check_file_length(f'{path_output_val}{dataset.split(\".p\")[0].split(\"/\")[-1]}_val.pkl', 1) \n",
    "\n",
    "pickle_data_in_from_to(path_output_train, 0, num_points-15, \"train_base_cut_3\", eta_cut=3,data=\"base\")\n",
    "pickle_data_in_from_to(path_output_train, 0, num_points, \"full_base_cut_3\", eta_cut=3,data=\"base\")\n",
    "pickle_data_in_from_to(path_output_val, num_points-15, num_points, \"val_base_base_cut_3\", eta_cut=3,data=\"base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the training of the emulators for all datasets\n",
    "\n",
    "After training the emulators, we save them with `dill`, such that they can be reloaded from file for the MCMC later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the training without the `parameterTrafoPCA` (for $\\zeta/s(T)$, $\\eta/s(\\mu_B)$, $\\langle y_{\\rm loss}\\rangle(y_{\\rm init})$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][emulator_BAND] loading training data from ../actual/latent_train/full_base_cut_3.pkl ...\n",
      "[INFO][emulator_BAND] Not discarding parameters with high errors. To enable this, we have to add cuts!\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 500, discarded points: 0\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 500 training points ...\n",
      "[INFO][emulator_BAND] loading training data from ../actual/latent_train/full_base_cut_3.pkl ...\n",
      "[INFO][emulator_BAND] Not discarding parameters with high errors. To enable this, we have to add cuts!\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 500, discarded points: 0\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 500 training points ...\n"
     ]
    }
   ],
   "source": [
    "#model_par contains the full range of values for the parameters\n",
    "model_par = parent+'/config_AuAu_200_bulk_scan_central.yaml'\n",
    "path_input =parent+'/latent_train/'\n",
    "path_output = parent+'/trained_emulators_no_PCA/'\n",
    "\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "emu1 = EmulatorBAND(f'{path_input}full_base_cut_3.pkl', model_par, method='PCGP', logTrafo=False, parameterTrafoPCA=False)\n",
    "\n",
    "emu1.trainEmulatorAutoMask()\n",
    "emu2 = EmulatorBAND(f'{path_input}full_base_cut_3.pkl', model_par, method='PCSK', logTrafo=False, parameterTrafoPCA=False)\n",
    "emu2.trainEmulatorAutoMask()\n",
    "with open(f'{path_output}PCGP_trained_full_base_cut_3.sav', 'wb') as f:\n",
    "    dill.dump(emu1, f)\n",
    "with open(f'{path_output}PCSK_trained_full_base_cut_3.sav', 'wb') as f:\n",
    "    dill.dump(emu2, f)\n",
    "\n",
    "\n",
    "#around 9 min per emulator (pcsk much more!)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the training with the `parameterTrafoPCA` (for $\\zeta/s(T)$, $\\eta/s(\\mu_B)$, $\\langle y_{\\rm loss}\\rangle(y_{\\rm init})$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][emulator_BAND] loading training data from ../latent_train/train.pkl ...\n",
      "[INFO][emulator_BAND] Not discarding parameters with high errors. To enable this, we have to add cuts!\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 4, discarded points: 0\n",
      "[INFO][emulator_BAND] Prepare bulk viscosity parameter PCA ...\n",
      "[INFO][emulator_BAND] Bulk viscosity parameter PCA uses 3 PCs to explain 99.0% of the variance ...\n",
      "[INFO][emulator_BAND] Prepare shear viscosity parameter PCA ...\n",
      "[INFO][emulator_BAND] Shear viscosity parameter PCA uses 1 PCs to explain 99.0% of the variance ...\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 4 training points ...\n",
      "[INFO][emulator_BAND] loading training data from ../latent_train/train.pkl ...\n",
      "[INFO][emulator_BAND] Not discarding parameters with high errors. To enable this, we have to add cuts!\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 4, discarded points: 0\n",
      "[INFO][emulator_BAND] Prepare bulk viscosity parameter PCA ...\n",
      "[INFO][emulator_BAND] Bulk viscosity parameter PCA uses 3 PCs to explain 99.0% of the variance ...\n",
      "[INFO][emulator_BAND] Prepare shear viscosity parameter PCA ...\n",
      "[INFO][emulator_BAND] Shear viscosity parameter PCA uses 1 PCs to explain 99.0% of the variance ...\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 4 training points ...\n"
     ]
    }
   ],
   "source": [
    "model_par = parent+'/config_AuAu_200_bulk_scan_central.yaml'\n",
    "path_input =parent+'/latent_train/'\n",
    "path_output = parent+'/trained_emulators_PCA/'\n",
    "\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "emu1 = EmulatorBAND(f'{path_input}full_base_cut_3.pkl', model_par, method='PCGP', logTrafo=False, parameterTrafoPCA=True)\n",
    "\n",
    "emu1.trainEmulatorAutoMask()\n",
    "emu2 = EmulatorBAND(f'{path_input}full_base_cut_3.pkl', model_par, method='PCSK', logTrafo=False, parameterTrafoPCA=True)\n",
    "emu2.trainEmulatorAutoMask()\n",
    "with open(f'{path_output}PCGP_trained_full_base_cut_3.sav', 'wb') as f:\n",
    "    dill.dump(emu1, f)\n",
    "with open(f'{path_output}PCSK_trained_full_base_cut_3.sav', 'wb') as f:\n",
    "    dill.dump(emu2, f)\n",
    "\n",
    "\n",
    "\n",
    "#TODO: HOW MANY PCA COMPONENTS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][emulator_BAND] loading training data from ../latent_train/train.pkl ...\n",
      "[INFO][emulator_BAND] Not discarding parameters with high errors. To enable this, we have to add cuts!\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 4, discarded points: 0\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 4 training points ...\n",
      "[INFO][emulator_BAND] loading training data from ../latent_train/train.pkl ...\n",
      "[INFO][emulator_BAND] Not discarding parameters with high errors. To enable this, we have to add cuts!\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 4, discarded points: 0\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 4 training points ...\n"
     ]
    }
   ],
   "source": [
    "model_par = parent+'/config_AuAu_200_bulk_scan_central.yaml'\n",
    "path_input =parent+'/latent_train/'\n",
    "path_output = parent+'/trained_emulators_LOG/'\n",
    "\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "emu1 = EmulatorBAND(f'{path_input}full_base_cut_3.pkl', model_par, method='PCGP', logTrafo=True, parameterTrafoPCA=False)\n",
    "\n",
    "emu1.trainEmulatorAutoMask()\n",
    "emu2 = EmulatorBAND(f'{path_input}full_base_cut_3.pkl', model_par, method='PCSK', logTrafo=True, parameterTrafoPCA=False)\n",
    "emu2.trainEmulatorAutoMask()\n",
    "with open(f'{path_output}PCGP_trained_full_base_cut_3.sav', 'wb') as f:\n",
    "    dill.dump(emu1, f)\n",
    "with open(f'{path_output}PCSK_trained_full_base_cut_3.sav', 'wb') as f:\n",
    "    dill.dump(emu2, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][emulator_BAND] loading training data from ../latent_train/train.pkl ...\n",
      "[INFO][emulator_BAND] Not discarding parameters with high errors. To enable this, we have to add cuts!\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 4, discarded points: 0\n",
      "[INFO][emulator_BAND] Prepare bulk viscosity parameter PCA ...\n",
      "[INFO][emulator_BAND] Bulk viscosity parameter PCA uses 3 PCs to explain 99.0% of the variance ...\n",
      "[INFO][emulator_BAND] Prepare shear viscosity parameter PCA ...\n",
      "[INFO][emulator_BAND] Shear viscosity parameter PCA uses 1 PCs to explain 99.0% of the variance ...\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 4 training points ...\n",
      "[INFO][emulator_BAND] loading training data from ../latent_train/train.pkl ...\n",
      "[INFO][emulator_BAND] Not discarding parameters with high errors. To enable this, we have to add cuts!\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 4, discarded points: 0\n",
      "[INFO][emulator_BAND] Prepare bulk viscosity parameter PCA ...\n",
      "[INFO][emulator_BAND] Bulk viscosity parameter PCA uses 3 PCs to explain 99.0% of the variance ...\n",
      "[INFO][emulator_BAND] Prepare shear viscosity parameter PCA ...\n",
      "[INFO][emulator_BAND] Shear viscosity parameter PCA uses 1 PCs to explain 99.0% of the variance ...\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 4 training points ...\n"
     ]
    }
   ],
   "source": [
    "model_par = parent+'/config_AuAu_200_bulk_scan_central.yaml'\n",
    "path_input =parent+'/latent_train/'\n",
    "path_output = parent+'/trained_emulators_LOG_PCA/'\n",
    "\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "emu1 = EmulatorBAND(f'{path_input}full.pkl', model_par, method='PCGP', logTrafo=True, parameterTrafoPCA=True)\n",
    "\n",
    "emu1.trainEmulatorAutoMask()\n",
    "emu2 = EmulatorBAND(f'{path_input}full.pkl', model_par, method='PCSK', logTrafo=True, parameterTrafoPCA=True)\n",
    "emu2.trainEmulatorAutoMask()\n",
    "with open(f'{path_output}PCGP_trained_full.sav', 'wb') as f:\n",
    "    dill.dump(emu1, f)\n",
    "with open(f'{path_output}PCSK_trained_full.sav', 'wb') as f:\n",
    "    dill.dump(emu2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate an 'experimental' dataset from one of the posterior points for closure testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_data = './separate_training_posterior_data_1095/'\n",
    "# path_output = './separate_training_posterior_data_1095/'\n",
    "# datasets_posterior = [\n",
    "#             'AuAu200_dNdy_posterior.pkl',\n",
    "#             'AuAu200_PHOBOSdNdeta_posterior.pkl',\n",
    "#             'AuAu200_PHOBOSv2eta_posterior.pkl',\n",
    "#             'AuAu200_pTvn_posterior.pkl',\n",
    "#             'AuAu19p6_dNdy_posterior.pkl',\n",
    "#             'AuAu19p6_PHOBOSdNdeta_posterior.pkl',\n",
    "#             'AuAu19p6_pTvn_posterior.pkl',\n",
    "#             'AuAu7.7_dNdy_posterior.pkl',\n",
    "#             'AuAu7.7_pTvn_posterior.pkl']\n",
    "\n",
    "# # Check if the output folder exists, if not, create it\n",
    "# if not os.path.exists(path_output):\n",
    "#     os.makedirs(path_output)\n",
    "\n",
    "# event_data = []\n",
    "# for dataset in datasets_posterior:\n",
    "#     current_path = path_data + dataset\n",
    "#     with open(current_path, \"rb\") as pf:\n",
    "#         data = pickle.load(pf)\n",
    "\n",
    "#     # Get the first event from the posterior dataset\n",
    "#     test_data = {event_id: data[event_id] for event_id in sorted(data.keys())[4:5]}\n",
    "#     event_data.append(test_data)\n",
    "\n",
    "\n",
    "# for event_dict in event_data[1:]:\n",
    "#     # Get the 'obs' array for the current event\n",
    "#     obs_array_new = event_dict['1099']['obs']\n",
    "    \n",
    "#     # Extend the 'obs' array of the first element with the values from the current event\n",
    "#     event_data[0]['1099']['obs'] = np.concatenate((event_data[0]['1099']['obs'], obs_array_new), axis=1)\n",
    "\n",
    "# # Save separated data to pickle files\n",
    "# with open(f'{path_output}example_data_test_point1099.pkl', 'wb') as pf1:\n",
    "#     pickle.dump(event_data[0], pf1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_data = './separate_training_posterior_data_1095/'\n",
    "# path_output = './separate_training_posterior_data_1095/'\n",
    "# datasets_posterior = [\n",
    "#             'AuAu200_dNdy_posterior.pkl',\n",
    "#             'AuAu200_PHOBOSdNdeta_posterior.pkl',\n",
    "#             'AuAu200_PHOBOSv2eta_posterior.pkl',\n",
    "#             'AuAu200_pTvn_posterior.pkl',\n",
    "#             'AuAu19p6_dNdy_posterior.pkl',\n",
    "#             'AuAu19p6_PHOBOSdNdeta_posterior.pkl',\n",
    "#             'AuAu19p6_pTvn_posterior.pkl',\n",
    "#             'AuAu7.7_dNdy_posterior.pkl',\n",
    "#             'AuAu7.7_pTvn_posterior.pkl']\n",
    "\n",
    "# # Check if the output folder exists, if not, create it\n",
    "# if not os.path.exists(path_output):\n",
    "#     os.makedirs(path_output)\n",
    "\n",
    "# event_data = []\n",
    "# for dataset in datasets_posterior:\n",
    "#     current_path = path_data + dataset\n",
    "#     with open(current_path, \"rb\") as pf:\n",
    "#         data = pickle.load(pf)\n",
    "\n",
    "#     # Get the first event from the posterior dataset\n",
    "#     test_data = {event_id: data[event_id] for event_id in sorted(data.keys())[2:3]}\n",
    "#     event_data.append(test_data)\n",
    "\n",
    "\n",
    "# for event_dict in event_data[1:]:\n",
    "#     # Get the 'obs' array for the current event\n",
    "#     obs_array_new = event_dict['1097']['obs']\n",
    "    \n",
    "#     # Extend the 'obs' array of the first element with the values from the current event\n",
    "#     event_data[0]['1097']['obs'] = np.concatenate((event_data[0]['1097']['obs'], obs_array_new), axis=1)\n",
    "\n",
    "# # Save separated data to pickle files\n",
    "# with open(f'{path_output}example_data_test_point1097.pkl', 'wb') as pf1:\n",
    "#     pickle.dump(event_data[0], pf1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the point for the test of the logarithmic training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = './separate_training_posterior_data_1095/'\n",
    "path_output = './separate_training_posterior_data_1095/'\n",
    "datasets_posterior = [\n",
    "            'AuAu200_dNdy_posterior.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu200_PHOBOSv2eta_posterior.pkl',\n",
    "            'AuAu200_pTvn_posterior.pkl',\n",
    "            'AuAu19p6_dNdy_posterior.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu19p6_pTvn_posterior.pkl',\n",
    "            'AuAu7.7_dNdy_posterior.pkl',\n",
    "            'AuAu7.7_pTvn_posterior.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "event_data = []\n",
    "for dataset in datasets_posterior:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "\n",
    "    # Get the first event from the posterior dataset\n",
    "    test_data = {event_id: data[event_id] for event_id in sorted(data.keys())[4:5]}\n",
    "    event_data.append(test_data)\n",
    "\n",
    "# modify the datasets with the multiplicities and take the log\n",
    "datasets_to_modify = [0,1,4,5,7]\n",
    "for i in datasets_to_modify:\n",
    "    event_data[i]['1099']['obs'][0,:] = np.log(np.abs(event_data[i]['1099']['obs'][0,:]) + 1e-30)\n",
    "    event_data[i]['1099']['obs'][1,:] = np.abs(event_data[i]['1099']['obs'][1,:]/event_data[i]['1099']['obs'][0,:] + 1e-30)\n",
    "\n",
    "for event_dict in event_data[1:]:\n",
    "    # Get the 'obs' array for the current event\n",
    "    obs_array_new = event_dict['1099']['obs']\n",
    "    \n",
    "    # Extend the 'obs' array of the first element with the values from the current event\n",
    "    event_data[0]['1099']['obs'] = np.concatenate((event_data[0]['1099']['obs'], obs_array_new), axis=1)\n",
    "\n",
    "# Save separated data to pickle files\n",
    "with open(f'{path_output}example_data_test_point1099_LOG.pkl', 'wb') as pf1:\n",
    "    pickle.dump(event_data[0], pf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate one dataset from all of the training and posterior points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../data/'\n",
    "path_output = './full_data_one_pkl/'\n",
    "datasets_posterior = [\n",
    "            'AuAu200_dNdy.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta.pkl',\n",
    "            'AuAu200_PHOBOSv2eta.pkl',\n",
    "            'AuAu200_pTvn.pkl',\n",
    "            'AuAu19p6_dNdy.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta.pkl',\n",
    "            'AuAu19p6_pTvn.pkl',\n",
    "            'AuAu7.7_dNdy.pkl',\n",
    "            'AuAu7.7_pTvn.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "event_data = []\n",
    "for dataset in datasets_posterior:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "\n",
    "    # Get the first event from the posterior dataset\n",
    "    sorted_keys = sorted(data.keys())\n",
    "    test_data = {event_id: data[event_id] for event_id in sorted_keys}\n",
    "    event_data.append(test_data)\n",
    "\n",
    "for dataset in event_data[1:]:\n",
    "    for event in sorted_keys:\n",
    "        # Get the 'obs' array for the current event\n",
    "        obs_array_new = dataset[event]['obs']\n",
    "        \n",
    "        # Extend the 'obs' array of the first dataset with the values from the others\n",
    "        event_data[0][event]['obs'] = np.concatenate((event_data[0][event]['obs'], obs_array_new), axis=1)\n",
    "\n",
    "# Save separated data to pickle files\n",
    "with open(f'{path_output}all_points_all_observables.pkl', 'wb') as pf1:\n",
    "    pickle.dump(event_data[0], pf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete parameters 16 and 17 from pkl files (bulk_max_rhob2,bulk_max_rhob4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../data/'\n",
    "path_output = '../data_new/'\n",
    "datasets = ['AuAu7.7_dNdy.pkl',\n",
    "            'AuAu7.7_pTvn.pkl',\n",
    "            'AuAu19p6_dNdy.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta.pkl',\n",
    "            'AuAu19p6_pTvn.pkl',\n",
    "            'AuAu200_dNdy.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta.pkl',\n",
    "            'AuAu200_PHOBOSv2eta.pkl',\n",
    "            'AuAu200_pTvn.pkl',\n",
    "            'AuAu7.7_logdNdy.pkl',\n",
    "            'AuAu19p6_logdNdy.pkl',\n",
    "            'AuAu19p6_logPHOBOSdNdeta.pkl',\n",
    "            'AuAu200_logdNdy.pkl',\n",
    "            'AuAu200_logPHOBOSdNdeta.pkl'\n",
    "            ]\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "for dataset in datasets:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "\n",
    "    print(f\"The dataset {dataset} has length {len(data)}.\")\n",
    "    # Separate data based on event ID\n",
    "    sorted_event_ids = sorted(data.keys(), key=lambda x: int(x))\n",
    "    first_data = {event_id: data[event_id] for event_id in sorted_event_ids}\n",
    "    print(\"Parameters before =\",len(first_data[f'{sorted_event_ids[0]}']['parameter']))\n",
    "\n",
    "    for point in range(len(sorted_event_ids)):\n",
    "        first_data[f'{sorted_event_ids[point]}']['parameter'] = np.delete(first_data[f'{sorted_event_ids[point]}']['parameter'], [16,17])\n",
    "\n",
    "    print(\"Parameters after =\",len(first_data[f'{sorted_event_ids[0]}']['parameter']))\n",
    "    # Save new data to pickle files\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}.pkl', 'wb') as pf1:\n",
    "        pickle.dump(first_data, pf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset in datasets:\n",
    "#     if \"integrated\" not in dataset:\n",
    "#         dict_df_full={}\n",
    "#         for i in range(num_points):\n",
    "#             current_path = path_data +\"/\"+str(i)+\"/\"+dataset\n",
    "#             config_path = path_configs +\"/config_sets_run_\"+str(i+1).zfill(3)+\".yaml\"\n",
    "#             df = pd.read_csv(current_path)\n",
    "#             dict_df={\"obs\":np.asarray([df.iloc[:, -2]*np.random.uniform(0.8, 2, df.shape[0])+np.random.uniform(0., 1e-10, df.shape[0]),df.iloc[:, -1]]), \"parameter\": read_config(config_path)}\n",
    "#             #print(read_config(config_path))\n",
    "#             dict_df_full[i]=dict_df\n",
    "#         with open( path_output+dataset+\".pkl\", 'wb') as f:\n",
    "#             pickle.dump(dict_df_full, f)\n",
    "#         list_of_pickles.append(path_output+dataset+\".pkl\")\n",
    "#     elif \"integrated\" in dataset:\n",
    "#         for key in integrated_values:\n",
    "#             dict_df_full={}\n",
    "#             for i in range(num_points):\n",
    "#                 current_path = path_data +\"/\"+str(i)+\"/\"+dataset\n",
    "#                 config_path = path_configs +\"/config_sets_run_\"+str(i+1).zfill(3)+\".yaml\"\n",
    "#                 df = pd.read_csv(current_path)\n",
    "#                 dict_df={\"obs\":np.asarray([df[key].values*random.uniform(0.8, 2),df[key+\"_error\"].values]), \"parameter\": read_config(config_path)}\n",
    "#                 dict_df_full[i]=dict_df\n",
    "#             with open(path_output + dataset + \"_\" + key + \".pkl\", 'wb') as f:\n",
    "#                 pickle.dump(dict_df_full, f)\n",
    "#             list_of_pickles.append(path_output+dataset+\"_\"+key+\".pkl\")\n",
    "#print(list_of_pickles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
