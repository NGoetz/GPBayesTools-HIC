{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: dill in /home/goetz/.local/lib/python3.10/site-packages (0.3.8)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: surmise in /home/goetz/.local/lib/python3.10/site-packages (0.2.1)\n",
      "Requirement already satisfied: scikit-learn>=1.2.0 in /home/goetz/.local/lib/python3.10/site-packages (from surmise) (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.18.3 in /usr/lib/python3/dist-packages (from surmise) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.7 in /usr/lib/python3/dist-packages (from surmise) (1.8.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/goetz/.local/lib/python3.10/site-packages (from scikit-learn>=1.2.0->surmise) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/lib/python3/dist-packages (from scikit-learn>=1.2.0->surmise) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "!pip install dill\n",
    "!pip install surmise\n",
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "import dill\n",
    "import pickle\n",
    "sys.path.insert(0, path.abspath('./'))\n",
    "\n",
    "from src.emulator_BAND import EmulatorBAND\n",
    "#from src.emulator import Emulator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to create pickle file with training dataset and validation test points\n",
    "\n",
    "Use the last 5 parameters as validation. Later we might add posterior points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAndom variation added!\n"
     ]
    }
   ],
   "source": [
    "def read_config(file):\n",
    "    with open(file, 'r') as f:\n",
    "        # Initialize an empty dictionary to store the values\n",
    "        values = []\n",
    "        # Read the file line by line\n",
    "        for line in f:\n",
    "            # If the line starts with a '#', it's a comment\n",
    "            if line.startswith('#'):\n",
    "                # Split the line at the colon\n",
    "                parts = line.split(':')\n",
    "                # The key is the part before the colon, stripped of leading/trailing whitespace and the '#'\n",
    "                key = parts[0].strip().lstrip('#')\n",
    "                # The value is the part after the colon, stripped of leading/trailing whitespace\n",
    "                value = parts[1].strip()\n",
    "                # Add the key-value pair to the dictionary\n",
    "                values.append(value)\n",
    "    return values\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "####################\n",
    "#GLOBAL VARIABLES\n",
    "#num_points=500\n",
    "num_points=4\n",
    "####################\n",
    "\n",
    "\n",
    "\n",
    "#generate the data pickles\n",
    "path_data = '../training_points/raw'\n",
    "path_configs = '../training_points/configs'\n",
    "path_output = '../latent_pickled/'\n",
    "datasets = [\n",
    "    'exp_19.6_05_eta_spectra', 'exp_200_05_y_spectra_piminus',\n",
    "    'exp_19.6_05_integrated', 'exp_200_05_y_spectra_piplus',\n",
    "    'exp_19.6_05_pT_spectra_kminus', 'exp_200_1525_eta_spectra',\n",
    "    'exp_19.6_05_pT_spectra_kplus', 'exp_200_1525_phobos_v2_spectra',\n",
    "    'exp_19.6_05_pT_spectra_p', 'exp_200_2030_integrated',\n",
    "    'exp_19.6_05_pT_spectra_pbar', 'exp_200_2030_phenix_pT_v2_spectra',\n",
    "    'exp_19.6_05_pT_spectra_piminus', 'exp_200_2030_phenix_pT_v3_spectra',\n",
    "    'exp_19.6_05_pT_spectra_piplus', 'exp_200_2030_pT_spectra_kminus',\n",
    "    'exp_19.6_1525_eta_spectra', 'exp_200_2030_pT_spectra_kplus','exp_200_2030_pT_spectra_p',\n",
    "    'exp_19.6_2030_integrated', 'exp_200_2030_pT_spectra_pbar',\n",
    "    'exp_19.6_2030_pT_spectra_kminus', 'exp_200_2030_pT_spectra_piminus',\n",
    "    'exp_19.6_2030_pT_spectra_kplus', 'exp_200_2030_pT_spectra_piplus',\n",
    "    'exp_19.6_2030_pT_spectra_p', 'exp_7.7_05_integrated',\n",
    "    'exp_19.6_2030_pT_spectra_pbar', 'exp_7.7_05_pT_spectra_kminus',\n",
    "    'exp_19.6_2030_pT_spectra_piminus', 'exp_7.7_05_pT_spectra_kplus',\n",
    "    'exp_19.6_2030_pT_spectra_piplus', 'exp_7.7_05_pT_spectra_p',\n",
    "    'exp_19.6_2030_star_v2_pT_spectra', 'exp_7.7_05_pT_spectra_pbar',\n",
    "    'exp_200_05_eta_spectra', 'exp_7.7_05_pT_spectra_piminus',\n",
    "    'exp_200_05_integrated', 'exp_7.7_05_pT_spectra_piplus',\n",
    "    'exp_200_05_pT_spectra_kminus', 'exp_7.7_2030_integrated',\n",
    "    'exp_200_05_pT_spectra_kplus', 'exp_7.7_2030_pT_spectra_kminus',\n",
    "    'exp_200_05_pT_spectra_p', 'exp_7.7_2030_pT_spectra_kplus',\n",
    "    'exp_200_05_pT_spectra_pbar', 'exp_7.7_2030_pT_spectra_p',\n",
    "    'exp_200_05_pT_spectra_piminus', 'exp_7.7_2030_pT_spectra_pbar',\n",
    "    'exp_200_05_pT_spectra_piplus', 'exp_7.7_2030_pT_spectra_piminus',\n",
    "    'exp_200_05_y_spectra_kminus', 'exp_7.7_2030_pT_spectra_piplus',\n",
    "    'exp_200_05_y_spectra_kplus', 'exp_7.7_2030_star_v2_pT_spectra'\n",
    "]\n",
    "list_of_pickles=[]\n",
    "integrated_values=[\n",
    "    \"dNdy_kminus\",\n",
    "    \"dNdy_kplus\",\n",
    "    \"dNdy_p\",\n",
    "    \"dNdy_pbar\",\n",
    "    \"dNdy_piminus\",\n",
    "    \"dNdy_piplus\",\n",
    "    \"meanpT_kminus\",\n",
    "    \"meanpT_kplus\",\n",
    "    \"meanpT_p\",\n",
    "    \"meanpT_pbar\",\n",
    "    \"meanpT_piminus\",\n",
    "    \"meanpT_piplus\",\n",
    "    \"star_v2\",\n",
    "    \"star_v3\"\n",
    "]\n",
    "print(\"RAndom variation added!\")\n",
    "import random \n",
    "for dataset in datasets:\n",
    "    if \"integrated\" not in dataset:\n",
    "        dict_df_full={}\n",
    "        for i in range(num_points):\n",
    "            current_path = path_data +\"/\"+str(i)+\"/\"+dataset\n",
    "            config_path = path_configs +\"/\"+str(i)+\"/config_sets_run_\"+str(i).zfill(3)+\".yaml\"\n",
    "            df = pd.read_csv(current_path)\n",
    "            dict_df={\"obs\":np.asarray([df.iloc[:, -2]*random.uniform(0.8, 2),df.iloc[:, -1]]), \"parameter\": read_config(current_path)}\n",
    "            dict_df_full[i]=dict_df\n",
    "        with open( path_output+dataset+\".pkl\", 'wb') as f:\n",
    "            pickle.dump(dict_df_full, f)\n",
    "        list_of_pickles.append(path_output+dataset+\".pkl\")\n",
    "    elif \"integrated\" in dataset:\n",
    "        for key in integrated_values:\n",
    "            dict_df_full={}\n",
    "            for i in range(num_points):\n",
    "                current_path = path_data +\"/\"+str(i)+\"/\"+dataset\n",
    "                config_path = path_configs +\"/config_sets_run_\"+str(i+1).zfill(3)+\".yaml\"\n",
    "                df = pd.read_csv(current_path)\n",
    "                dict_df={\"obs\":np.asarray([df[key].values*random.uniform(0.8, 2),df[key+\"_error\"].values]), \"parameter\": read_config(current_path)}\n",
    "                dict_df_full[i]=dict_df\n",
    "            with open(path_output + dataset + \"_\" + key + \".pkl\", 'wb') as f:\n",
    "                pickle.dump(dict_df_full, f)\n",
    "            list_of_pickles.append(path_output+dataset+\"_\"+key+\".pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do they have to be pickels?\n",
    "#the pickels contain dictonaries. the dictionary is [{obs: [[val1, val2...], [err1,err2...]], parameter: i}] with length design points\n",
    "\"\"\"\n",
    "dataDict = {\n",
    "    'event_id1': { #id of parameter points\n",
    "        'obs': [[...], [...]],  # 2D array-like structure, values and errors\n",
    "        'parameter': ...  # parameter list, just pure numerical values\n",
    "    },\n",
    "    'event_id2': {\n",
    "        'obs': [[...], [...]],  # 2D array-like structure\n",
    "        'parameter': ...  # Scalar or 1D array\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "\"\"\"\n",
    "#print(\"The following pickles have been created:\")\n",
    "datasets = list_of_pickles\n",
    "path_output_train = '../latent_train/'\n",
    "path_output_val = '../latent_val/'\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output_train):\n",
    "    os.makedirs(path_output_train)\n",
    "\n",
    "if not os.path.exists(path_output_val ):\n",
    "    os.makedirs(path_output_val )\n",
    "\n",
    "def check_file_length(filename, expected_length):\n",
    "    with open(f\"{path_output}{filename}\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    if len(data) == expected_length:\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"{filename} does not have the correct length. Expected: {expected_length}, Actual: {len(data)}\")\n",
    "\n",
    "datasets_train = []\n",
    "for dataset in datasets:\n",
    "    current_path = dataset\n",
    "    #print(dataset)\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "    \n",
    "    #print(f\"The dataset {dataset} has length {len(data)}.\")\n",
    "    # Separate data based on event ID\n",
    "    sorted_event_ids = sorted(data.keys(), key=lambda x: int(x))\n",
    "    first_data = {event_id: data[event_id] for event_id in sorted_event_ids[:num_points-1]}\n",
    "    second_data = {event_id: data[event_id] for event_id in sorted_event_ids[num_points-1:]}\n",
    "    #print(f'{dataset.split(\".p\")[0].split(\"/\")[-1]}_train.pkl')\n",
    "    # Save separated data to pickle files\n",
    "    with open(f'{path_output_train}{dataset.split(\".p\")[0].split(\"/\")[-1]}_train.pkl', 'wb') as pf1:\n",
    "        pickle.dump(first_data, pf1)\n",
    "        \n",
    "    with open(f'{path_output_val}{dataset.split(\".p\")[0].split(\"/\")[-1]}_val.pkl', 'wb') as pf2:\n",
    "        pickle.dump(second_data, pf2)\n",
    "\n",
    "    check_file_length(f'{path_output_train}{dataset.split(\".p\")[0].split(\"/\")[-1]}_train.pkl', num_points-1)\n",
    "    datasets_train.append(f'{path_output_train}{dataset.split(\".p\")[0].split(\"/\")[-1]}_train.pkl')\n",
    "    check_file_length(f'{path_output_val}{dataset.split(\".p\")[0].split(\"/\")[-1]}_val.pkl', 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the training of the emulators for all datasets\n",
    "\n",
    "After training the emulators, we save them with `dill`, such that they can be reloaded from file for the MCMC later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the training without the `parameterTrafoPCA` (for $\\zeta/s(T)$, $\\eta/s(\\mu_B)$, $\\langle y_{\\rm loss}\\rangle(y_{\\rm init})$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][emulator_BAND] loading training data from ../latent_train/../latent_train/exp_19.6_05_eta_spectra_train.pkl ...\n",
      "[INFO][emulator_BAND] Not discarding parameters with high errors. To enable this, we have to add cuts!\n",
      "[INFO][emulator_BAND] All training data are loaded.\n",
      "[INFO][emulator_BAND] Training dataset size: 3, discarded points: 0\n",
      "162\n",
      "[  2.01549308   2.60223077   3.62171472   4.71268659   5.72692269\n",
      "   6.96181734   9.3089277   12.10386962  15.90326628  21.17003891\n",
      "  28.33767517  36.98209735  45.63031214  57.6298207   71.3060643\n",
      "  86.48823191 100.14954254 112.97214299 125.19596488 136.16815182\n",
      " 143.07705641 147.68075942 150.99919165 153.79599119 155.78480108\n",
      " 156.33924307 155.57334693 155.65917103 154.35815237 152.89382325\n",
      " 151.84894933 150.77629873 147.7374788  140.78963927 133.42451457\n",
      " 124.15596158 112.11104279  98.7632013   84.35641281  70.01037328\n",
      "  57.1161432   46.08122249  38.05647063  30.37871202  23.0314332\n",
      "  17.63901601  12.98255454   9.79802578   7.55681588   5.63443033\n",
      "   4.02168697   3.28881245   2.54198381   2.08358836]\n",
      "[INFO][emulator_BAND] Performing emulator training ...\n",
      "[INFO][emulator_BAND] Train GP emulators with 3 training points ...\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_70299/2937964641.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mstd_devs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memu1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd_devs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0memu1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainEmulatorAutoMask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0memu2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmulatorBAND\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path_input}{dataset}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_par\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'PCSK'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogTrafo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameterTrafoPCA\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0memu2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainEmulatorAutoMask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/good_bayes/inference/GPBayesTools-HIC/src/emulator_BAND.py\u001b[0m in \u001b[0;36mtrainEmulatorAutoMask\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrainEmulatorAutoMask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mtrainEventMask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainEmulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainEventMask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/good_bayes/inference/GPBayesTools-HIC/src/emulator_BAND.py\u001b[0m in \u001b[0;36mtrainEmulator\u001b[0;34m(self, event_mask)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'PCGP'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m             self.emu = emulator(x=X,theta=design_points,\n\u001b[0m\u001b[1;32m    273\u001b[0m                             \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevent_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'PCGP'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/surmise/emulation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, theta, f, method, passthroughfunc, args, options)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__f\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'autofit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/surmise/emulation.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0margstemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margstemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/surmise/emulationmethods/PCGP.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(fitinfo, x, theta, f, epsilon, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# fit a GP for each PC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpcanum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpcs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0memulist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpcanum\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memulation_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitinfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpcanum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mfitinfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'emulist'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memulist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/surmise/emulationmethods/PCGP.py\u001b[0m in \u001b[0;36memulation_fit\u001b[0;34m(theta, pcaval)\u001b[0m\n\u001b[1;32m    394\u001b[0m     bounds = spo.Bounds(np.append(covhypLB, nughypLB),\n\u001b[1;32m    395\u001b[0m                         np.append(covhypUB, nughypUB))\n\u001b[0;32m--> 396\u001b[0;31m     opval = spo.minimize(emulation_negloglik,\n\u001b[0m\u001b[1;32m    397\u001b[0m                          \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcovhyp0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnughyp0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m                          \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    679\u001b[0m                                  **options)\n\u001b[1;32m    680\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m         res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[1;32m    682\u001b[0m                                callback=callback, **options)\n\u001b[1;32m    683\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/scipy/optimize/_lbfgsb_py.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0miprint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m     sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n\u001b[0m\u001b[1;32m    309\u001b[0m                                   \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_bounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                                   finite_diff_rel_step=finite_diff_rel_step)\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;31m# calculation reduces overall function evaluations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     sf = ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0m\u001b[1;32m    264\u001b[0m                         finite_diff_rel_step, bounds, epsilon=epsilon)\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m# Hessian Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_grad\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mupdate_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mFD_METHODS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mgrad_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mgrad_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mupdate_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/surmise/emulationmethods/PCGP.py\u001b[0m in \u001b[0;36memulation_negloglikgrad\u001b[0;34m(hyperparameters, fitinfo)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;34m/\u001b[0m \u001b[0mmudenom\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mmuhat\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdonespincalc\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mdRnorm\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mdonespincalc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmudenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0mdsigma2hat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdfcentercalc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mdRnorm\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mdfcentercalc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdmuhat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfcenter\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0monespin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0mdlogdet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRinv\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdRnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "#model_par contains the full range of values for the parameters\n",
    "model_par = '../training_points/configs/config_AuAu_200_bulk_scan_central.yaml'\n",
    "path_input = '../latent_train/'\n",
    "path_output = '../trained_emulators_no_PCA/'\n",
    "\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "for dataset in datasets_train:\n",
    "    emu1 = EmulatorBAND(f'{path_input}{dataset}', model_par, method='PCGP', logTrafo=False, parameterTrafoPCA=False)\n",
    "    print( emu1.model_data.size)\n",
    "    std_devs = np.std(emu1.model_data, axis=0)\n",
    "    print(std_devs)\n",
    "    emu1.trainEmulatorAutoMask()\n",
    "    emu2 = EmulatorBAND(f'{path_input}{dataset}', model_par, method='PCSK', logTrafo=False, parameterTrafoPCA=False)\n",
    "    emu2.trainEmulatorAutoMask()\n",
    "    #emu3 = Emulator(f'{path_input}{dataset}', model_par, npc = 4, logTrafo=False, parameterTrafoPCA=False)\n",
    "   # emu3.trainEmulatorAutoMask()\n",
    "\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_PCGP_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu1, f)\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_PCSK_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the training with the `parameterTrafoPCA` (for $\\zeta/s(T)$, $\\eta/s(\\mu_B)$, $\\langle y_{\\rm loss}\\rangle(y_{\\rm init})$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_par = '../data/modelDesign_3DMCGlauber.txt'\n",
    "path_input = './separate_training_posterior_data/'\n",
    "path_output = './trained_emulators_PCA/'\n",
    "\n",
    "datasets_train = ['AuAu7.7_dNdy_train.pkl',\n",
    "            'AuAu7.7_pTvn_train.pkl',\n",
    "            'AuAu19p6_dNdy_train.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta_train.pkl',\n",
    "            'AuAu19p6_pTvn_train.pkl',\n",
    "            'AuAu200_dNdy_train.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta_train.pkl',\n",
    "            'AuAu200_PHOBOSv2eta_train.pkl',\n",
    "            'AuAu200_pTvn_train.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "for dataset in datasets_train:\n",
    "    emu1 = EmulatorBAND(f'{path_input}{dataset}', model_par, method='PCGP', logTrafo=False, parameterTrafoPCA=True)\n",
    "    emu1.trainEmulatorAutoMask()\n",
    "    emu2 = EmulatorBAND(f'{path_input}{dataset}', model_par, method='PCSK', logTrafo=False, parameterTrafoPCA=True)\n",
    "    emu2.trainEmulatorAutoMask()\n",
    "    \n",
    "\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_PCGP_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu1, f)\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_PCSK_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu2, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate an 'experimental' dataset from one of the posterior points for closure testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = './separate_training_posterior_data_1095/'\n",
    "path_output = './separate_training_posterior_data_1095/'\n",
    "datasets_posterior = [\n",
    "            'AuAu200_dNdy_posterior.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu200_PHOBOSv2eta_posterior.pkl',\n",
    "            'AuAu200_pTvn_posterior.pkl',\n",
    "            'AuAu19p6_dNdy_posterior.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu19p6_pTvn_posterior.pkl',\n",
    "            'AuAu7.7_dNdy_posterior.pkl',\n",
    "            'AuAu7.7_pTvn_posterior.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "event_data = []\n",
    "for dataset in datasets_posterior:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "\n",
    "    # Get the first event from the posterior dataset\n",
    "    test_data = {event_id: data[event_id] for event_id in sorted(data.keys())[4:5]}\n",
    "    event_data.append(test_data)\n",
    "\n",
    "\n",
    "for event_dict in event_data[1:]:\n",
    "    # Get the 'obs' array for the current event\n",
    "    obs_array_new = event_dict['1099']['obs']\n",
    "    \n",
    "    # Extend the 'obs' array of the first element with the values from the current event\n",
    "    event_data[0]['1099']['obs'] = np.concatenate((event_data[0]['1099']['obs'], obs_array_new), axis=1)\n",
    "\n",
    "# Save separated data to pickle files\n",
    "with open(f'{path_output}example_data_test_point1099.pkl', 'wb') as pf1:\n",
    "    pickle.dump(event_data[0], pf1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = './separate_training_posterior_data_1095/'\n",
    "path_output = './separate_training_posterior_data_1095/'\n",
    "datasets_posterior = [\n",
    "            'AuAu200_dNdy_posterior.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu200_PHOBOSv2eta_posterior.pkl',\n",
    "            'AuAu200_pTvn_posterior.pkl',\n",
    "            'AuAu19p6_dNdy_posterior.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu19p6_pTvn_posterior.pkl',\n",
    "            'AuAu7.7_dNdy_posterior.pkl',\n",
    "            'AuAu7.7_pTvn_posterior.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "event_data = []\n",
    "for dataset in datasets_posterior:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "\n",
    "    # Get the first event from the posterior dataset\n",
    "    test_data = {event_id: data[event_id] for event_id in sorted(data.keys())[2:3]}\n",
    "    event_data.append(test_data)\n",
    "\n",
    "\n",
    "for event_dict in event_data[1:]:\n",
    "    # Get the 'obs' array for the current event\n",
    "    obs_array_new = event_dict['1097']['obs']\n",
    "    \n",
    "    # Extend the 'obs' array of the first element with the values from the current event\n",
    "    event_data[0]['1097']['obs'] = np.concatenate((event_data[0]['1097']['obs'], obs_array_new), axis=1)\n",
    "\n",
    "# Save separated data to pickle files\n",
    "with open(f'{path_output}example_data_test_point1097.pkl', 'wb') as pf1:\n",
    "    pickle.dump(event_data[0], pf1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the point for the test of the logarithmic training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = './separate_training_posterior_data_1095/'\n",
    "path_output = './separate_training_posterior_data_1095/'\n",
    "datasets_posterior = [\n",
    "            'AuAu200_dNdy_posterior.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu200_PHOBOSv2eta_posterior.pkl',\n",
    "            'AuAu200_pTvn_posterior.pkl',\n",
    "            'AuAu19p6_dNdy_posterior.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu19p6_pTvn_posterior.pkl',\n",
    "            'AuAu7.7_dNdy_posterior.pkl',\n",
    "            'AuAu7.7_pTvn_posterior.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "event_data = []\n",
    "for dataset in datasets_posterior:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "\n",
    "    # Get the first event from the posterior dataset\n",
    "    test_data = {event_id: data[event_id] for event_id in sorted(data.keys())[4:5]}\n",
    "    event_data.append(test_data)\n",
    "\n",
    "# modify the datasets with the multiplicities and take the log\n",
    "datasets_to_modify = [0,1,4,5,7]\n",
    "for i in datasets_to_modify:\n",
    "    event_data[i]['1099']['obs'][0,:] = np.log(np.abs(event_data[i]['1099']['obs'][0,:]) + 1e-30)\n",
    "    event_data[i]['1099']['obs'][1,:] = np.abs(event_data[i]['1099']['obs'][1,:]/event_data[i]['1099']['obs'][0,:] + 1e-30)\n",
    "\n",
    "for event_dict in event_data[1:]:\n",
    "    # Get the 'obs' array for the current event\n",
    "    obs_array_new = event_dict['1099']['obs']\n",
    "    \n",
    "    # Extend the 'obs' array of the first element with the values from the current event\n",
    "    event_data[0]['1099']['obs'] = np.concatenate((event_data[0]['1099']['obs'], obs_array_new), axis=1)\n",
    "\n",
    "# Save separated data to pickle files\n",
    "with open(f'{path_output}example_data_test_point1099_LOG.pkl', 'wb') as pf1:\n",
    "    pickle.dump(event_data[0], pf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate one dataset from all of the training and posterior points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../data/'\n",
    "path_output = './full_data_one_pkl/'\n",
    "datasets_posterior = [\n",
    "            'AuAu200_dNdy.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta.pkl',\n",
    "            'AuAu200_PHOBOSv2eta.pkl',\n",
    "            'AuAu200_pTvn.pkl',\n",
    "            'AuAu19p6_dNdy.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta.pkl',\n",
    "            'AuAu19p6_pTvn.pkl',\n",
    "            'AuAu7.7_dNdy.pkl',\n",
    "            'AuAu7.7_pTvn.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "event_data = []\n",
    "for dataset in datasets_posterior:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "\n",
    "    # Get the first event from the posterior dataset\n",
    "    sorted_keys = sorted(data.keys())\n",
    "    test_data = {event_id: data[event_id] for event_id in sorted_keys}\n",
    "    event_data.append(test_data)\n",
    "\n",
    "for dataset in event_data[1:]:\n",
    "    for event in sorted_keys:\n",
    "        # Get the 'obs' array for the current event\n",
    "        obs_array_new = dataset[event]['obs']\n",
    "        \n",
    "        # Extend the 'obs' array of the first dataset with the values from the others\n",
    "        event_data[0][event]['obs'] = np.concatenate((event_data[0][event]['obs'], obs_array_new), axis=1)\n",
    "\n",
    "# Save separated data to pickle files\n",
    "with open(f'{path_output}all_points_all_observables.pkl', 'wb') as pf1:\n",
    "    pickle.dump(event_data[0], pf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete parameters 16 and 17 from pkl files (bulk_max_rhob2,bulk_max_rhob4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../data/'\n",
    "path_output = '../data_new/'\n",
    "datasets = ['AuAu7.7_dNdy.pkl',\n",
    "            'AuAu7.7_pTvn.pkl',\n",
    "            'AuAu19p6_dNdy.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta.pkl',\n",
    "            'AuAu19p6_pTvn.pkl',\n",
    "            'AuAu200_dNdy.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta.pkl',\n",
    "            'AuAu200_PHOBOSv2eta.pkl',\n",
    "            'AuAu200_pTvn.pkl',\n",
    "            'AuAu7.7_logdNdy.pkl',\n",
    "            'AuAu19p6_logdNdy.pkl',\n",
    "            'AuAu19p6_logPHOBOSdNdeta.pkl',\n",
    "            'AuAu200_logdNdy.pkl',\n",
    "            'AuAu200_logPHOBOSdNdeta.pkl'\n",
    "            ]\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "for dataset in datasets:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "\n",
    "    print(f\"The dataset {dataset} has length {len(data)}.\")\n",
    "    # Separate data based on event ID\n",
    "    sorted_event_ids = sorted(data.keys(), key=lambda x: int(x))\n",
    "    first_data = {event_id: data[event_id] for event_id in sorted_event_ids}\n",
    "    print(\"Parameters before =\",len(first_data[f'{sorted_event_ids[0]}']['parameter']))\n",
    "\n",
    "    for point in range(len(sorted_event_ids)):\n",
    "        first_data[f'{sorted_event_ids[point]}']['parameter'] = np.delete(first_data[f'{sorted_event_ids[point]}']['parameter'], [16,17])\n",
    "\n",
    "    print(\"Parameters after =\",len(first_data[f'{sorted_event_ids[0]}']['parameter']))\n",
    "    # Save new data to pickle files\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}.pkl', 'wb') as pf1:\n",
    "        pickle.dump(first_data, pf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
