{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "from glob import glob\n",
    "import sys\n",
    "import csv\n",
    "sys.path.insert(0, path.abspath('./'))\n",
    "\n",
    "from src import workdir, parse_model_parameter_file\n",
    "from src.emulator_BAND import EmulatorBAND\n",
    "#from src.emulator import Emulator\n",
    "\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_abs_prediction_err(emu_pred,vali_true):\n",
    "    rms_abs_pred_err = np.zeros(emu_pred.shape[1])\n",
    "    for obsIdx in range(emu_pred.shape[1]):\n",
    "        quantity = np.zeros(emu_pred.shape[1])\n",
    "        for testpoint in range(emu_pred.shape[0]):\n",
    "            quantity[obsIdx] += ((emu_pred[testpoint,obsIdx] - vali_true[testpoint,obsIdx]) / vali_true[testpoint,obsIdx])**2.\n",
    "        rms_abs_pred_err[obsIdx] = np.sqrt(quantity[obsIdx] / emu_pred.shape[0])\n",
    "    return rms_abs_pred_err\n",
    "\n",
    "def how_honest_is_GP(emu_pred,emu_pred_err,vali_true):\n",
    "    rms_quantity = np.zeros(emu_pred.shape[1])\n",
    "    for obsIdx in range(emu_pred.shape[1]):\n",
    "        quantity = np.zeros(emu_pred.shape[1])\n",
    "        for testpoint in range(emu_pred.shape[0]):\n",
    "            quantity[obsIdx] += ((emu_pred[testpoint,obsIdx] - vali_true[testpoint,obsIdx]) / emu_pred_err[testpoint,obsIdx])**2.\n",
    "        rms_quantity[obsIdx] = np.sqrt(quantity[obsIdx] / emu_pred.shape[0])\n",
    "    return rms_quantity\n",
    "\n",
    "def train_multiple_emulators(training_set, model_par, number_test_points, logFlag, parameterTrafoPCAFlag):\n",
    "    emu1 = EmulatorBAND(training_set, model_par, method='PCGP', logTrafo=logFlag, parameterTrafoPCA=parameterTrafoPCAFlag)\n",
    "    emu2 = EmulatorBAND(training_set, model_par, method='PCSK', logTrafo=logFlag, parameterTrafoPCA=parameterTrafoPCAFlag)\n",
    "    #emu3 = Emulator(training_set, model_par, npc = 4, logTrafo=logFlag, parameterTrafoPCA=parameterTrafoPCAFlag)\n",
    "\n",
    "    output_emu1 = emu1.testEmulatorErrors(number_test_points=number_test_points)\n",
    "    emu_pred_1 = output_emu1[0]\n",
    "    emu_pred_err_1 = output_emu1[1]\n",
    "    vali_data_1 = output_emu1[2]\n",
    "    vali_data_err_1 = output_emu1[3]\n",
    "\n",
    "    output_emu2 = emu2.testEmulatorErrors(number_test_points=number_test_points)\n",
    "    emu_pred_2 = output_emu2[0]\n",
    "    emu_pred_err_2 = output_emu2[1]\n",
    "    vali_data_2 = output_emu2[2]\n",
    "    vali_data_err_2 = output_emu2[3]\n",
    "\n",
    "    # output_emu3 = emu3.testEmulatorErrors(nTestPoints=number_test_points)\n",
    "    # emu_pred_3 = output_emu3[0]\n",
    "    # emu_pred_err_3 = output_emu3[1]\n",
    "    # vali_data_3 = output_emu3[2]\n",
    "    # vali_data_err_3 = output_emu3[3]\n",
    "\n",
    "    nObs = vali_data_1.shape[1]  # Assuming all datasets have the same number of observables\n",
    "\n",
    "    rms_abs_pred_err1 = rms_abs_prediction_err(emu_pred_1,vali_data_1)\n",
    "    rms_abs_pred_err2 = rms_abs_prediction_err(emu_pred_2,vali_data_2)\n",
    "    #rms_abs_pred_err3 = rms_abs_prediction_err(emu_pred_3,vali_data_3)\n",
    "    honesty_1 = how_honest_is_GP(emu_pred_1,emu_pred_err_1,vali_data_1)\n",
    "    honesty_2 = how_honest_is_GP(emu_pred_2,emu_pred_err_2,vali_data_2)\n",
    "    #honesty_3 = how_honest_is_GP(emu_pred_3,emu_pred_err_3,vali_data_3)\n",
    "\n",
    "    return (rms_abs_pred_err1,rms_abs_pred_err2), (honesty_1,honesty_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_emulator_file_errors(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(float(line.strip()))\n",
    "    return data\n",
    "\n",
    "def read_multiple_emulator_errors_files(number_test_points_list,foldername,filename):\n",
    "    data_list1 = []\n",
    "    data_list2 = []\n",
    "\n",
    "    for i in number_test_points_list:\n",
    "        filename1 = f\"{foldername}/{filename}_{i}_pred_err_obs_1.dat\"\n",
    "        data1 = read_emulator_file_errors(filename1)\n",
    "        data_list1.append(data1)\n",
    "\n",
    "        filename2 = f\"{foldername}/{filename}_{i}_pred_err_obs_2.dat\"\n",
    "        data2 = read_emulator_file_errors(filename2)\n",
    "        data_list2.append(data2)\n",
    "\n",
    "        # filename3 = f\"./{foldername}/{filename}_{i}_pred_err_obs_3.dat\"\n",
    "        # data3 = read_emulator_file_errors(filename3)\n",
    "        # data_list3.append(data3)\n",
    "    \n",
    "    data_list4 = []\n",
    "    data_list5 = []\n",
    "\n",
    "    for i in number_test_points_list:\n",
    "        filename4 = f\"./{foldername}/{filename}_{i}_GP_honesty_obs_1.dat\"\n",
    "        data4 = read_emulator_file_errors(filename4)\n",
    "        data_list4.append(data4)\n",
    "\n",
    "        filename5 = f\"./{foldername}/{filename}_{i}_GP_honesty_obs_2.dat\"\n",
    "        data5 = read_emulator_file_errors(filename5)\n",
    "        data_list5.append(data5)\n",
    "\n",
    "    return (data_list1,data_list2), (data_list4,data_list5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def convert_to_notation(filename):\n",
    "    # Remove 'exp_', '../latent_pickled/', and '.pkl'\n",
    "    filename = re.sub(r'(exp_|\\.pkl|../latent_pickled/)', '', filename)\n",
    "    filename = re.sub(r'star_v(.*)_pT', r'starv\\1pt', filename)\n",
    "    filename = re.sub(r'star_v', 'starv', filename)\n",
    "    filename = re.sub(r'phenix_pT_v', 'phenixptv', filename)\n",
    "    \n",
    "    filename = re.sub(r'eta_spectra', 'etas', filename)\n",
    "    filename = re.sub(r'y_spectra', 'ys', filename)\n",
    "    filename = re.sub(r'pT_spectra', 'pTs', filename)\n",
    "    filename = re.sub(r'phobos_v2_spectra', 'v2eta', filename)\n",
    "    # Mapping for observables\n",
    "    observables = {\n",
    "        'etas': r'$\\frac{dN}{d\\eta}$',\n",
    "        'ys': r'$\\frac{dN}{dy}$',\n",
    "        'pTs': r'$\\frac{dN}{d(p_T)}$',\n",
    "        'dNdy': r'$\\left.\\frac{dN}{dy}\\right|_{y=0}$',\n",
    "        'meanpT': r'$\\langle p_T \\rangle |_{y=0}$',\n",
    "        'starv2': r'$\\left.v_2\\right.\\{2\\}$',\n",
    "        'starv3': r'$\\left.v_3\\right.\\{2\\}$',\n",
    "        'phenixptv2': r'$v_2(p_T)$',\n",
    "        'phenixptv3': r'$v_3(p_T)$',\n",
    "        'starv2pt': r'$v_2(p_T)$',\n",
    "        'starv3pt': r'$v_3(p_T)$',\n",
    "        'v2eta': r'$v_2(\\eta)$'\n",
    "    }\n",
    "    particles= {\n",
    "        'kminus': r'K^-',\n",
    "        'kplus': r'K^+',\n",
    "        'p': 'p',\n",
    "        'pbar': r'\\bar{p}',\n",
    "        'piminus': r'\\pi^-',\n",
    "        'piplus': r'\\pi^+'\n",
    "    }   \n",
    "    \n",
    "    # Identify the particle and the observable\n",
    "    #print(filename)\n",
    "    parts = filename.split('_')\n",
    "    #print(parts)\n",
    "    particle = ''\n",
    "    observable = ''\n",
    "    integrated = False\n",
    "    for part in parts:\n",
    "        if part in observables:\n",
    "            observable = observables[part]\n",
    "        elif part in particles:\n",
    "            particle = particles[part]\n",
    "        elif part == 'integrated':\n",
    "            integrated = True\n",
    "    # Add particle notation for y_spectra\n",
    "    if integrated and particle == 'p':\n",
    "        pass\n",
    "    elif integrated and particle != '':\n",
    "        observable=''\n",
    "    elif ('ys' in filename or 'pTs' in filename or integrated) and not 'star' in filename:\n",
    "        observable = observable[:-1] + f'({particle})$'\n",
    "\n",
    "   \n",
    "    #print(observable)\n",
    "    #print(\"------\")\n",
    "    # Return the formatted string\n",
    "    return observable\n",
    "\n",
    "print(convert_to_notation('star_v2_pT_spectra.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['r', 'b', 'g', 'magenta', 'teal', 'orange', 'purple', 'grey', 'seagreen']\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(40, 20))\n",
    "plt.rc('font', size=18)\n",
    "\n",
    "energies = ['7.7', '19.6', '200']\n",
    "centralities = ['05', '1525', '2030']\n",
    "id_max=[-1]\n",
    "for i, energy in enumerate(energies):\n",
    "    axs[i].set_title(f'Energy: {energy} GeV')\n",
    "   \n",
    "    dataset_boundaries = [0]\n",
    "    cent_boundaries = [0]\n",
    "    idx_observable = 0\n",
    "    midpoint=[]\n",
    "    cent=[]\n",
    "    for centrality in centralities:\n",
    "        \n",
    "        if centrality in data[energy]:\n",
    "            cent_boundaries.append(idx_observable)\n",
    "            for dataset in range(len(data[energy][centrality][\"name\"])):\n",
    "                #idx = filename_prefix_list.index(f'exp_{energy}_{centrality}_{data[energy][centrality][dataset]}')\n",
    "                # if(energy=='200'):\n",
    "                #     print(data[energy][centrality][dataset])\n",
    "                #     print(len(err1_list[dataset][-1]))\n",
    "                #     print(\"-----\")\n",
    "                #print(data[energy][centrality][\"name\"][dataset])\n",
    "                for obs in range(len(data[energy][centrality][\"err1\"][dataset])):\n",
    "                    axs[i].scatter(idx_observable, np.abs(data[energy][centrality][\"err1\"][dataset][obs]), color=colors[0], marker='o', s=10)\n",
    "                    axs[i].scatter(idx_observable, np.abs(data[energy][centrality][\"err2\"][dataset][obs]), color=colors[1], marker='s', s=10)\n",
    "                    idx_observable += 1\n",
    "                    #print(obs)\n",
    "                    #print(dataset)\n",
    "                    #print(data[energy][centrality][dataset])\n",
    "                    \n",
    "                idx_observable+=1\n",
    "                dataset_boundaries.append(idx_observable)\n",
    "            \n",
    "            axs[i].axvline(x=idx_observable, color='k', linestyle='--', zorder=(-10))\n",
    "            if not (energy=='7.7' and centrality=='05'):\n",
    "                idx_observable+=1\n",
    "            #idx_observable+=2\n",
    "            if not (energy=='7.7' and centrality=='1525'):\n",
    "                midpoint.append((idx_observable + cent_boundaries[-1]) / 2)\n",
    "                cent.append(centrality)\n",
    "            \n",
    "    id_max.append(idx_observable)\n",
    "    for k,mid in enumerate(midpoint):\n",
    "        axs[i].text(mid/idx_observable, 0.82, cent[k], rotation=0, verticalalignment='bottom', horizontalalignment='center',transform=axs[i].transAxes)\n",
    "        \n",
    "    \n",
    "    average_indices = [(start + end) // 2  - 1 for start, end in zip(dataset_boundaries[:-1], dataset_boundaries[1:])]\n",
    "    #print(average_indices)\n",
    "    #print(average_indices)\n",
    "    axs[i].set_xticks(average_indices)\n",
    "    labels = [convert_to_notation(f'{obs}') for centrality in centralities for obs in data[energy][centrality][\"name\"]]\n",
    "    #print(labels)\n",
    "    #print(labels)\n",
    "    #print(labels)\n",
    "    axs[i].set_xticklabels(labels, rotation=45, ha='right', fontsize=10)\n",
    "\n",
    "    #for boundary, cent in zip(dataset_boundaries[:-1], centralities):\n",
    "        \n",
    "\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], marker='o', color=colors[0], markersize=10, label='PCGP', linestyle='None'),\n",
    "    plt.Line2D([0], [0], marker='s', color=colors[1], markersize=10, label='PCSK', linestyle='None'),\n",
    "]\n",
    "\n",
    "axs[0].legend(handles=legend_elements, loc='upper right', borderpad=0.12, borderaxespad=0.25)\n",
    "\n",
    "for i,ax in enumerate(axs):\n",
    "    ax.set_xlim([-1, id_max[i+1] + 1])\n",
    "    ax.set_ylabel(r\"$\\mathcal{E}$\")\n",
    "    ax.set_yscale('log')\n",
    "    ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: '{:g}'.format(x)))\n",
    "    ax.yaxis.set_major_locator(ticker.LogLocator(subs=[1.0, 2.0, 5.0]))  # Ensure at least two labels\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"err_full_training_set_LHCpoints_noPCA.pdf\")\n",
    "\n",
    "# # honesty plot\n",
    "fig, axs = plt.subplots(3, 1, figsize=(40, 20))\n",
    "plt.rc('font', size=18)\n",
    "\n",
    "energies = ['7.7', '19.6', '200']\n",
    "centralities = ['05', '1525', '2030']\n",
    "\n",
    "for i, energy in enumerate(energies):\n",
    "    axs[i].set_title(f'Energy: {energy} GeV')\n",
    "   \n",
    "    dataset_boundaries = [0]\n",
    "    cent_boundaries = [0]\n",
    "    idx_observable = 0\n",
    "    midpoint=[]\n",
    "    cent=[]\n",
    "    for centrality in centralities:\n",
    "        \n",
    "        if centrality in data[energy]:\n",
    "            cent_boundaries.append(idx_observable)\n",
    "            for dataset in range(len(data[energy][centrality][\"name\"])):\n",
    "                #idx = filename_prefix_list.index(f'exp_{energy}_{centrality}_{data[energy][centrality][dataset]}')\n",
    "                # if(energy=='200'):\n",
    "                #     print(data[energy][centrality][dataset])\n",
    "                #     print(len(err1_list[dataset][-1]))\n",
    "                #     print(\"-----\")\n",
    "                #print(data[energy][centrality][\"name\"][dataset])\n",
    "                for obs in range(len(data[energy][centrality][\"err1\"][dataset])):\n",
    "                    axs[i].scatter(idx_observable, np.abs(data[energy][centrality][\"hon1\"][dataset][obs]), color=colors[0], marker='o', s=10)\n",
    "                    axs[i].scatter(idx_observable, np.abs(data[energy][centrality][\"hon2\"][dataset][obs]), color=colors[1], marker='s', s=10)\n",
    "                    idx_observable += 1\n",
    "                    #print(obs)\n",
    "                    #print(dataset)\n",
    "                    #print(data[energy][centrality][dataset])\n",
    "                    \n",
    "                idx_observable+=1\n",
    "                dataset_boundaries.append(idx_observable)\n",
    "            \n",
    "            axs[i].axvline(x=idx_observable, color='k', linestyle='--', zorder=(-10))\n",
    "            if not (energy=='7.7' and centrality=='05'):\n",
    "                idx_observable+=1\n",
    "            #idx_observable+=2\n",
    "            if not (energy=='7.7' and centrality=='1525'):\n",
    "                midpoint.append((idx_observable + cent_boundaries[-1]) / 2)\n",
    "                cent.append(centrality)\n",
    "            \n",
    "    id_max.append(idx_observable)\n",
    "    for k,mid in enumerate(midpoint):\n",
    "        axs[i].text(mid/idx_observable, 0.82, cent[k], rotation=0, verticalalignment='bottom', horizontalalignment='center',transform=axs[i].transAxes)\n",
    "    \n",
    "    average_indices = [(start + end) // 2  - 1 for start, end in zip(dataset_boundaries[:-1], dataset_boundaries[1:])]\n",
    "    #print(average_indices)\n",
    "    #print(average_indices)\n",
    "    axs[i].set_xticks(average_indices)\n",
    "    labels = [convert_to_notation(f'{obs}') for centrality in centralities for obs in data[energy][centrality][\"name\"]]\n",
    "    #print(labels)\n",
    "    #print(labels)\n",
    "    #print(labels)\n",
    "    axs[i].set_xticklabels(labels, rotation=45, ha='right', fontsize=10)\n",
    "\n",
    "    #for boundary, cent in zip(dataset_boundaries[:-1], centralities):\n",
    "        \n",
    "\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], marker='o', color=colors[0], markersize=10, label='PCGP', linestyle='None'),\n",
    "    plt.Line2D([0], [0], marker='s', color=colors[1], markersize=10, label='PCSK', linestyle='None'),\n",
    "]\n",
    "\n",
    "axs[0].legend(handles=legend_elements, loc='upper right', borderpad=0.12, borderaxespad=0.25)\n",
    "\n",
    "for i,ax in enumerate(axs):\n",
    "    ax.set_xlim([-1, id_max[i+1] + 1])\n",
    "    ax.set_ylabel(r\"$\\mathcal{E}$\")\n",
    "    ax.set_yscale('log')\n",
    "    ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: '{:g}'.format(x)))\n",
    "    ax.yaxis.set_major_locator(ticker.LogLocator(subs=[1.0, 2.0, 5.0]))  # Ensure at least two labels\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"hon_full_training_set_LHCpoints_noPCA.pdf\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
